# Code Documentation - Sentiment Analysis Project

**Author:** Amrani Bouabdellah  
**Course:** Introduction to Deep Learning - 5th Year Statistics and Data Science  
**Institution:** ENSSEA  
**Instructor:** Ayoub Asri  
**Date:** February 7, 2026

---

## Purpose of This Document

This document explains what each component of the code does and the specific tools/libraries used in the implementation. It serves as a technical guide to understand the project structure and implementation details.

---

## 1. Data Processing Components

### 1.1 `src/data/preprocessing.py`

**Purpose:** Cleans and prepares raw text data for model input.

**What it does:**
- Removes URLs, HTML tags, and special characters from text
- Converts text to lowercase and removes extra whitespace
- Tokenizes text into words using NLTK
- Builds vocabulary from training data (max 10,000 words)
- Converts text to numerical sequences
- Pads/truncates sequences to fixed length (100 tokens)

**Tools used:**
- `nltk.tokenize` - Word tokenization
- `re` (regex) - Pattern matching for cleaning
- `string` - Text manipulation utilities
- Custom vocabulary dictionary mapping words to indices

**Example functionality:**
```python
# Input: "Check out this link: http://example.com!!! ðŸ˜Š"
# Output: [45, 123, 67, 891, ...] (numerical sequence)
```

### 1.2 `src/data/augmentation.py`

**Purpose:** Increases training data diversity through text transformations.

**What it does:**
- **Synonym replacement:** Swaps words with synonyms from WordNet
- **Random swap:** Randomly exchanges positions of words
- **Random deletion:** Removes random words with probability p
- Applies augmentation only to minority classes to balance dataset

**Tools used:**
- `nltk.corpus.wordnet` - Synonym dictionary
- `random` - Random sampling and selection
- NumPy - Array manipulation

**Example:**
```python
# Original: "This product is amazing"
# Augmented: "This product is fantastic" (synonym replacement)
# Augmented: "Product this is amazing" (random swap)
```

### 1.3 `src/data/dataset.py`

**Purpose:** Creates PyTorch dataset objects for efficient data loading.

**What it does:**
- Wraps preprocessed data into PyTorch Dataset class
- Handles batching and shuffling during training
- Integrates VADER sentiment scores as additional features
- Converts data to PyTorch tensors

**Tools used:**
- `torch.utils.data.Dataset` - Base dataset class
- `torch.utils.data.DataLoader` - Batch loading
- `vaderSentiment` - Sentiment feature extraction
- `torch.tensor` - Tensor conversion

---

## 2. Model Components

### 2.1 Baseline Models (`src/models/baseline/`)

#### 2.1.1 `fasttext.py`

**Purpose:** Implements FastText-style text classification.

**What it does:**
- Creates word embeddings (100 dimensions per word)
- Generates character n-grams (3-6 characters) for each word
- Averages all embeddings for each text
- Passes through linear layers for classification

**Architecture:**
```
Input text â†’ Embedding Layer â†’ Character N-grams â†’ Average Pooling â†’ 
Linear(100â†’128) â†’ ReLU â†’ Dropout â†’ Linear(128â†’3) â†’ Output
```

**Tools used:**
- `torch.nn.Embedding` - Word embedding layer
- `torch.nn.Linear` - Fully connected layers
- `torch.nn.Dropout` - Regularization (p=0.3)
- Custom n-gram generation function

**Parameters:** ~1 million

#### 2.1.2 `bilstm_attention.py`

**Purpose:** Bidirectional LSTM with multi-head self-attention mechanism.

**What it does:**
- Processes text sequentially in both directions (forward/backward)
- Applies 4-head self-attention to capture important words
- Integrates VADER sentiment scores as auxiliary features
- Uses layer normalization for training stability

**Architecture:**
```
Input â†’ Embedding â†’ BiLSTM(256 hidden) â†’ Multi-Head Attention(4 heads) â†’ 
Layer Norm â†’ Concatenate with VADER features â†’ 
Linear(512â†’256) â†’ ReLU â†’ Dropout â†’ Linear(256â†’3) â†’ Output
```

**Tools used:**
- `torch.nn.LSTM` - Bidirectional LSTM (2 layers)
- `torch.nn.MultiheadAttention` - Self-attention mechanism
- `torch.nn.LayerNorm` - Layer normalization
- `torch.cat` - Feature concatenation

**Parameters:** ~2.5 million

**Key innovation:** Combines sequential processing (LSTM) with attention mechanism to focus on important words.

#### 2.1.3 `custom_transformer.py`

**Purpose:** Transformer encoder built from scratch (not pretrained).

**What it does:**
- Adds positional information to embeddings (sine/cosine encoding)
- Applies 4 layers of transformer encoder blocks
- Each block has multi-head attention + feed-forward network
- Uses residual connections and layer normalization

**Architecture:**
```
Input â†’ Embedding + Positional Encoding â†’ 
[Encoder Block Ã— 4:
  - Multi-Head Attention (4 heads)
  - Add & Norm
  - Feed-Forward (256â†’1024â†’256)
  - Add & Norm
] â†’ Global Average Pooling â†’ Linear(256â†’3) â†’ Output
```

**Tools used:**
- `torch.nn.TransformerEncoder` - Transformer encoder
- `torch.nn.TransformerEncoderLayer` - Individual encoder layer
- Custom positional encoding (sin/cos functions)
- `torch.nn.LayerNorm` - Normalization

**Parameters:** ~3 million

**Mathematical operations:**
- Self-Attention: `Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V`
- Positional Encoding: `PE(pos,2i) = sin(pos/10000^(2i/d_model))`

### 2.2 Pretrained Models (`src/models/pretrained/`)

#### 2.2.1 `roberta.py`

**Purpose:** Fine-tunes RoBERTa (Robustly Optimized BERT) for sentiment classification.

**What it does:**
- Loads pretrained `roberta-base` model (125M parameters)
- Uses pretrained tokenizer for text encoding
- Adds custom classification head on top
- Fine-tunes entire model on sentiment data

**Architecture:**
```
Input Text â†’ RoBERTa Tokenizer â†’ RoBERTa Base (12 layers) â†’ 
[CLS] token representation â†’ 
Linear(768â†’384) â†’ ReLU â†’ LayerNorm â†’ Dropout(0.5) â†’
Linear(384â†’192) â†’ ReLU â†’ Dropout(0.5) â†’
Linear(192â†’3) â†’ Output
```

**Tools used:**
- `transformers.RobertaModel` - Pretrained model
- `transformers.RobertaTokenizer` - Subword tokenization
- `transformers.AdamW` - Optimizer with weight decay
- `transformers.get_linear_schedule_with_warmup` - Learning rate scheduler

**Training specifics:**
- Learning rate: 1e-5
- Warmup steps: 500
- Max length: 128 tokens
- Batch size: 16

#### 2.2.2 `bertweet.py`

**Purpose:** Fine-tunes BERTweet (BERT pretrained on Twitter data).

**What it does:**
- Loads `vinai/bertweet-base` model (135M parameters)
- Model already understands Twitter language (hashtags, mentions, emojis)
- Adds simpler 2-layer classification head
- Fine-tunes on sentiment dataset

**Architecture:**
```
Input Text â†’ BERTweet Tokenizer â†’ BERTweet Base â†’ 
[CLS] representation â†’ 
Linear(768â†’256) â†’ ReLU â†’ Dropout(0.5) â†’
Linear(256â†’3) â†’ Output
```

**Tools used:**
- `transformers.AutoModel` - BERTweet model
- `transformers.AutoTokenizer` - Twitter-aware tokenizer
- Same training tools as RoBERTa

**Why it performs best:** Pretrained specifically on 850M English tweets, understands informal language.

### 2.3 Ensemble Models (`src/models/ensemble/`)

#### 2.3.1 `voting_ensemble.py`

**Purpose:** Combines predictions from multiple models through voting.

**What it does:**
- **Soft voting:** Averages probability distributions from all models
- **Hard voting:** Takes majority class vote
- **Weighted voting:** Weights each model by validation accuracy

**Process:**
```python
# For each test sample:
# 1. Get predictions from all 5 models
# 2. Average probabilities: P_final = (P_model1 + P_model2 + ... + P_model5) / 5
# 3. Choose class with highest average probability
```

**Tools used:**
- `torch.stack` - Combine model outputs
- `torch.mean` - Average probabilities
- `numpy.argmax` - Select final class

#### 2.3.2 `stacking_ensemble.py`

**Purpose:** Meta-learning approach that learns how to combine model predictions.

**What it does:**
- Uses predictions from all models as **features**
- Trains a new neural network (meta-learner) to combine these features
- Meta-learner learns which model to trust for which samples

**Architecture:**
```
[Model1_probs, Model2_probs, ..., Model5_probs] â†’ 
Concatenate (15 features) â†’
Linear(15â†’64) â†’ ReLU â†’ Dropout(0.3) â†’
Linear(64â†’32) â†’ ReLU â†’ Dropout(0.3) â†’
Linear(32â†’3) â†’ Output
```

**Tools used:**
- All base models for feature generation
- `torch.nn` - Meta-learner network
- Train on validation set to avoid overfitting

**Training process:**
1. Train base models on training set
2. Generate predictions on validation set
3. Train meta-learner using these predictions as input
4. Test on separate test set

---

## 3. Training Components

### 3.1 `src/training/trainer.py`

**Purpose:** Handles the training loop and model optimization.

**What it does:**
- Implements training epoch loop (forward pass, loss calculation, backpropagation)
- Validation evaluation after each epoch
- Early stopping to prevent overfitting
- Model checkpoint saving (saves best model based on validation accuracy)
- Learning rate scheduling

**Process per epoch:**
```python
for batch in train_loader:
    1. Forward pass: predictions = model(batch)
    2. Calculate loss: loss = criterion(predictions, labels)
    3. Backward pass: loss.backward()
    4. Update weights: optimizer.step()
    5. Reset gradients: optimizer.zero_grad()
```

**Tools used:**
- `torch.optim.Adam` / `torch.optim.AdamW` - Optimizers
- `torch.nn.utils.clip_grad_norm_` - Gradient clipping
- Custom early stopping class
- `torch.save` / `torch.load` - Model checkpointing

### 3.2 `src/training/losses.py`

**Purpose:** Custom loss functions for handling class imbalance.

**What it does:**

#### Focal Loss
- Reduces loss for well-classified examples
- Focuses training on hard examples
- Formula: `FL(p_t) = -Î±(1-p_t)^Î³ * log(p_t)`
- Î³=2.0 in this project

#### Label Smoothing
- Prevents overconfident predictions
- Replaces hard labels [0,0,1] with soft labels [0.05, 0.05, 0.9]
- Îµ=0.1 smoothing factor

**Tools used:**
- `torch.nn.functional.cross_entropy` - Base loss
- `torch.nn.functional.softmax` - Probability conversion
- Custom implementation of focal loss formula

### 3.3 `src/training/metrics.py`

**Purpose:** Calculates evaluation metrics during training.

**What it does:**
- **Accuracy:** Percentage of correct predictions
- **Precision:** True positives / (True positives + False positives)
- **Recall:** True positives / (True positives + False negatives)
- **F1-Score:** Harmonic mean of precision and recall
- **MCC:** Matthews Correlation Coefficient (handles imbalance)
- **Confusion Matrix:** Shows prediction patterns

**Tools used:**
- `sklearn.metrics.accuracy_score`
- `sklearn.metrics.precision_recall_fscore_support`
- `sklearn.metrics.matthews_corrcoef`
- `sklearn.metrics.confusion_matrix`
- `sklearn.metrics.classification_report`

---

## 4. Evaluation Components

### 4.1 `src/evaluation/evaluator.py`

**Purpose:** Comprehensive model evaluation on test set.

**What it does:**
- Loads trained model from checkpoint
- Runs inference on test set
- Calculates all metrics (accuracy, F1, MCC, etc.)
- Generates per-class performance reports
- Saves evaluation results to JSON/CSV

**Tools used:**
- `torch.no_grad()` - Disable gradient computation for inference
- `sklearn.metrics` - All evaluation metrics
- `pandas` - Results formatting
- `json` - Results serialization

### 4.2 `src/evaluation/error_analysis.py`

**Purpose:** Analyzes misclassified examples to understand model failures.

**What it does:**
- Identifies all incorrectly predicted samples
- Categorizes errors by confusion type (e.g., Neutralâ†’Positive)
- Finds high-confidence errors (model was sure but wrong)
- Analyzes patterns (short texts, specific words, etc.)
- Generates error reports with examples

**Example output:**
```
Neutral â†’ Positive errors: 234 samples
Common patterns:
  - Short affirmative phrases: "Great!", "Thanks"
  - Average confidence: 0.87
  
High-confidence error example:
  Text: "Amazing and helpful"
  True: Neutral, Predicted: Positive (confidence: 0.95)
```

**Tools used:**
- `numpy` - Array operations for filtering
- `pandas` - Error categorization and analysis
- Custom pattern detection functions

### 4.3 `src/evaluation/visualizer.py`

**Purpose:** Creates visualizations of results.

**What it does:**
- **Confusion matrices:** Heatmaps showing prediction patterns
- **Training curves:** Loss and accuracy over epochs
- **Class distribution:** Bar charts of dataset balance
- **Performance comparison:** Bar charts comparing all models
- **Error distribution:** Analysis of mistake patterns

**Tools used:**
- `matplotlib.pyplot` - Basic plotting
- `seaborn` - Statistical visualizations and heatmaps
- `numpy` - Data preparation for plots
- `sklearn.metrics.ConfusionMatrixDisplay` - Confusion matrix plots

---

## 5. Utility Components

### 5.1 `src/utils/config.py`

**Purpose:** Centralized configuration management.

**What it does:**
- Defines all hyperparameters in one place
- Stores file paths and directory locations
- Model architecture specifications
- Training parameters (learning rate, batch size, epochs)

**Example configuration:**
```python
CONFIG = {
    'vocab_size': 10000,
    'embedding_dim': 100,
    'max_length': 100,
    'batch_size': 32,
    'learning_rate': 0.001,
    'num_epochs': 20,
    'device': 'cuda' if torch.cuda.is_available() else 'cpu'
}
```

**Tools used:**
- Python dictionaries or dataclasses
- `argparse` - Command-line argument parsing

### 5.2 `src/utils/logger.py`

**Purpose:** Logging system for tracking experiments.

**What it does:**
- Records training progress to log files
- Timestamps all events
- Logs hyperparameters, metrics, and errors
- Creates separate log files for each training run

**Tools used:**
- `logging` - Python logging module
- `datetime` - Timestamp generation
- File I/O for log persistence

### 5.3 `src/utils/helpers.py`

**Purpose:** Common utility functions used across project.

**What it does:**
- Set random seeds for reproducibility
- Device management (CPU/GPU selection)
- File operations (create directories, check paths)
- Data format conversions

**Example functions:**
```python
def set_seed(seed=42):
    """Ensures reproducible results"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```

**Tools used:**
- `random`, `numpy.random`, `torch.manual_seed` - Seed setting
- `os`, `pathlib` - File system operations
- `torch.cuda` - GPU utilities

### 5.4 `src/utils/model_loader.py`

**Purpose:** Loads saved models for inference or continued training.

**What it does:**
- Loads model architectures
- Restores trained weights from checkpoints
- Handles model versioning
- Ensures compatibility between save/load

**Tools used:**
- `torch.load` - Load model state
- `model.load_state_dict()` - Restore weights
- `pickle` - Serialization of auxiliary objects

---

## 6. Execution Scripts

### 6.1 `scripts/00_eda.py`

**Purpose:** Exploratory Data Analysis of the dataset.

**What it does:**
- Loads raw CSV data
- Computes statistics (text length, class distribution)
- Generates word clouds for each sentiment class
- Creates visualization plots
- Calculates VADER sentiment scores
- Saves analysis results and plots

**Tools used:**
- `pandas` - Data loading and analysis
- `matplotlib`, `seaborn` - Visualizations
- `wordcloud` - Word cloud generation
- `vaderSentiment` - Sentiment scoring

### 6.2 `scripts/01_train_baseline.py`

**Purpose:** Trains all baseline models (FastText, BiLSTM, Transformer).

**What it does:**
- Loads and preprocesses data
- Applies data augmentation if specified
- Trains each baseline model sequentially
- Saves trained models and training logs
- Generates performance metrics

**Command-line arguments:**
```bash
--data_path: Path to dataset CSV
--output_dir: Where to save results
--augmentation: Enable/disable data augmentation
--device: 'cuda' or 'cpu'
--epochs: Number of training epochs
```

**Tools used:**
- `argparse` - Parse command-line arguments
- All baseline model classes
- Training utilities

### 6.3 `scripts/02_train_pretrained.py`

**Purpose:** Fine-tunes pretrained transformer models.

**What it does:**
- Downloads pretrained RoBERTa and BERTweet models
- Tokenizes data with model-specific tokenizers
- Fine-tunes models with appropriate learning rates
- Implements early stopping and checkpointing
- Saves best models based on validation performance

**Key difference from baseline training:**
- Much smaller learning rate (1e-5 vs 0.001)
- Uses AdamW optimizer with weight decay
- Linear warmup schedule
- Fewer epochs needed (3-5 vs 20-25)

**Tools used:**
- `transformers` library - Model loading and training
- `torch` - Training loop
- Custom evaluation functions

### 6.4 `scripts/03_train_ensemble.py`

**Purpose:** Creates and trains ensemble models.

**What it does:**
- Loads all previously trained baseline and pretrained models
- Generates predictions on validation set
- Trains voting ensembles (hard, soft, weighted)
- Trains stacking meta-learner
- Evaluates all ensemble methods
- Saves ensemble models and results

**Process:**
1. Load 5 base models
2. Create validation predictions for each
3. Train meta-learner on these predictions
4. Evaluate on test set
5. Compare ensemble vs individual models

**Tools used:**
- Model loading utilities
- Ensemble classes
- Evaluation metrics

### 6.5 `scripts/04_evaluate_all.py`

**Purpose:** Comprehensive evaluation comparing all models.

**What it does:**
- Loads all trained models (baseline + pretrained + ensemble)
- Runs each model on the same test set
- Computes all metrics for fair comparison
- Generates comparison tables and plots
- Creates final performance report

**Output:**
- Comparison table (CSV)
- Performance plots (PNG)
- Statistical significance tests
- Best model identification

**Tools used:**
- All evaluation utilities
- `pandas` - Results aggregation
- `matplotlib` - Comparison plots

### 6.6 `scripts/05_error_analysis.py`

**Purpose:** Detailed analysis of model errors.

**What it does:**
- Loads best performing model (BERTweet)
- Identifies all misclassifications
- Categorizes errors by type
- Finds common error patterns
- Generates error examples with explanations
- Saves detailed error report

**Analysis includes:**
- Confusion patterns (which classes confused most)
- Text length analysis of errors
- High-confidence mistakes
- Difficult examples requiring manual review

**Tools used:**
- Error analysis utilities
- Statistical analysis functions
- Text pattern matching

---

## 7. Debugging Scripts

### 7.1 `debugging/FIX_ensemble_results.py`

**Purpose:** Fixes issues with ensemble model result saving.

**What it does:**
- Reloads ensemble models
- Re-runs evaluation
- Ensures results are properly saved
- Validates output format

### 7.2 `debugging/patch_models.py`

**Purpose:** Updates model code without retraining.

**What it does:**
- Loads existing model checkpoints
- Applies architecture fixes
- Resaves models with corrections

### 7.3 `debugging/save_processed_data.py`

**Purpose:** Saves preprocessed data for faster loading.

**What it does:**
- Runs full preprocessing pipeline
- Saves tokenized and padded sequences
- Saves vocabulary and label mappings
- Enables quick data loading in future runs

**Tools used:**
- `pickle` - Python object serialization
- `numpy.save` - Array saving

---

## Key Libraries and Their Roles

| Library | Version | Purpose |
|---------|---------|---------|
| **PyTorch** | â‰¥2.0.0 | Deep learning framework - all neural networks |
| **Transformers** | â‰¥4.30.0 | Pretrained models (RoBERTa, BERTweet) |
| **scikit-learn** | â‰¥1.3.0 | Evaluation metrics and utilities |
| **pandas** | â‰¥2.0.0 | Data manipulation and analysis |
| **NumPy** | â‰¥1.24.0 | Numerical operations and arrays |
| **NLTK** | â‰¥3.8.1 | Text tokenization and WordNet |
| **vaderSentiment** | â‰¥3.3.2 | Sentiment feature extraction |
| **Matplotlib** | â‰¥3.7.0 | Basic plotting and visualization |
| **Seaborn** | â‰¥0.12.0 | Statistical visualizations |

---

## Hardware and Performance

**Training Environment:**
- **GPU:** NVIDIA GPU with CUDA support (recommended)
- **RAM:** 16GB minimum for transformer models
- **Storage:** 5GB for models and data

**Training Times (on single GPU):**
- FastText: ~5 minutes
- BiLSTM-Attention: ~15 minutes
- Custom Transformer: ~25 minutes
- RoBERTa: ~45 minutes (5 epochs)
- BERTweet: ~50 minutes (4 epochs)

**Inference Speed (samples/second):**
- FastText: ~5,000
- BiLSTM: ~2,000
- Transformers: ~500
- Ensembles: ~100

---

## Reproducibility

All experiments are reproducible by:
1. Setting fixed random seeds (42)
2. Using deterministic algorithms
3. Documenting all hyperparameters
4. Version-controlling all code
5. Saving complete training logs

To reproduce results:
```bash
# Install exact dependencies
pip install -r requirements.txt

# Run training pipeline
python scripts/01_train_baseline.py --device cuda
python scripts/02_train_pretrained.py --device cuda
python scripts/03_train_ensemble.py --device cuda
python scripts/04_evaluate_all.py
```

---

## Summary

This project implements a complete sentiment analysis pipeline using:
- **3 baseline models** built from scratch
- **2 pretrained transformer models** fine-tuned on the task
- **3 ensemble methods** combining model predictions
- Comprehensive preprocessing, training, and evaluation frameworks
- Tools ranging from basic Python libraries to advanced transformer frameworks

The modular structure allows easy experimentation with different architectures, hyperparameters, and ensemble strategies while maintaining clean, reproducible code.
